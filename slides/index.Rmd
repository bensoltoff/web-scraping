---
title: "Gathering data from the web: Scraping with `rvest`"
author: "Benjamin Soltoff <br /> University of Chicago"
output: rcfss::xaringan
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, htmltools.preserve.raw = FALSE)
knitr::opts_chunk$set(
  cache = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.retina = 2, fig.width = 12
)

library(tidyverse)
library(rvest)
library(lubridate)
library(countdown)

set.seed(1234)
theme_set(theme_minimal(base_size = rcfss::base_size))
```

# Methods for obtaining data online

* Standalone files
* Application programming interface (API)
* **Scraping**

---

# Web scraping

* Data on a website with no API
* Still want a programmatic, reproducible way to obtain data
* Ability to scrape depends on the quality of the website

---

# HTML

.center[

![[tags](https://xkcd.com/1144/)](http://imgs.xkcd.com/comics/tags.png)

]

---

# Process of HTML

1. The web browser sends a request to the server that hosts the website
1. The server sends the browser an HTML document
1. The browser uses instructions in the HTML to render the website

---

# Components of HTML code

```html
<html>
  <head>
    <title>Title</title>
    <link rel="icon" type="icon" href="http://a" />
    <script src="https://c.js"></script>
  </head>
  <body>
    <div>
      <p>Click <b>here</b> now.</p>
      <span>Frozen</span>
    </div>
    <table style="width:100%">
      <tr>
        <td>Kristen</td>
        <td>Bell</td>
      </tr>
    </table>
  <img src="http://ia.media-imdb.com/images.png"/>
  </body>
</html>
```

---

# Components of HTML code

```html
<a href="http://github.com">GitHub</a>
```

* `<a></a>` - tag name
* `href` - attribute (argument)
* `"http://github.com"` - attribute (value)
* `GitHub` - content

---

# Nested structure of HTML

* `html`
    * `head`
        * `title`
        * `link`
        * `script`
    * `body`
        * `div`
            * `p`
                * `b`
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
        * `img`

---

# Find the content "here"

* `html`
    * `head`
        * `title`
        * `link`
        * `script`
    * `body`
        * `div`
            * `p`
                * <span style="color:red">**`b`**</span>
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
        * `img`

---

# HTML only

![HTML only](https://cfss.uchicago.edu/img/shiny-css-none.png)

---

# HTML + CSS

![HTML + CSS](https://cfss.uchicago.edu/img/shiny-css.png)

---

# CSS code

```css
span {
  color: #ffffff;
}

.num {
  color: #a8660d;
}

table.data {
  width: auto;
}

#firstname {
  background-color: yellow;
}
```

---

# CSS code

```html
<span class="bigname" id="shiny">Shiny</span>
```

* `<span></span>` - tag name
* `bigname` - class (optional)
* `shiny` - id (optional)

---

# CSS selectors

```css
span
```

```css
.bigname
```

```css
span.bigname
```

```css
#shiny
```

---

# CSS selectors

Prefix | Matches
-------|--------
none   | tag
.      | class
#      | id

> [CSS diner](http://flukeout.github.io)

---

# Find the CSS selector

.pull-left[

```html
<body>
    <table id="content">
        <tr class='name'>
            <td class='firstname'>
                Kurtis
            </td>
            <td class='lastname'>
                McCoy
            </td>
        </tr>
        <tr class='name'>
            <td class='firstname'>
                Leah
            </td>
            <td class='lastname'>
                Guerrero
            </td>
        </tr>
    </table>
</body>
```

]

.pull-right[

1. The entire table
1. Just the element containing first names

]

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 3)
```

---

# Scraping presidential statements

.center[

[![](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Dwight_D._Eisenhower%2C_official_photo_portrait%2C_May_29%2C_1959.jpg/613px-Dwight_D._Eisenhower%2C_official_photo_portrait%2C_May_29%2C_1959.jpg)](https://www.presidency.ucsb.edu/advanced-search?field-keywords=%22space+exploration%22&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&items_per_page=100)

]

---

# Using `rvest` to read HTML

1. Collect the HTML source code of a webpage
2. Read the HTML of the page
3. Select and keep certain elements of the page that are of interest

---

# Get the page

```{r get-statement}
dwight <- read_html(x = "https://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration")
dwight
```

---

# Find page elements

`rvest` lets you find elements by

1. HTML tags
1. HTML attributes
1. CSS selectors

---

# Find `a` elements

```{r nodes, dependson = "get-statement"}
html_elements(x = dwight, css = "a")
```

---

# SelectorGadget

* GUI tool used to identify CSS selector combinations from a webpage
1. Read [here](https://rvest.tidyverse.org/articles/articles/selectorgadget.html)
1. Drag **SelectorGadget** link into your browser's bookmark bar

---

# Using SelectorGadget

1. Navigate to a webpage
1. Open the SelectorGadget bookmark
1. Click on the item to scrape
1. Click on yellow items you do not want to scrape
1. Click on additional items that you do want to scrape
1. Rinse and repeat until only the items you want to scrape are highlighted in yellow
1. Copy the selector to use with `html_elements()`

---
# Find the CSS selector

Use Selector Gadget to find the CSS selector for the document's *speaker*.

Then, modify an argument in `html_elements` to look for this more specific CSS selector.

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 3)
```

---

# Find the CSS selector

```{r get-speaker, dependson = "get-statement"}
html_elements(x = dwight, css = ".diet-title a")
```

---

# Get attributes and text of elements

```{r get-speaker-text, dependson = "get-statement"}
# identify element with speaker name
speaker <- html_elements(dwight, ".diet-title a") %>% 
  html_text2() # Select text of element

speaker
```

---

# Get attributes and text of elements

```{r get-speaker-link, dependson = "get-statement"}
speaker_link <- html_elements(dwight, ".diet-title a") %>% 
  html_attr("href")

speaker_link
```

---

# Date of statement

```{r get-date, dependson = "get-statement"}
date <- html_elements(x = dwight, css = ".date-display-single") %>%
  html_text2() %>% # Grab element text
  mdy() # Format using lubridate
date
```

---

# Speaker name

```{r get-speaker-2, dependson = "get-statement"}
speaker <- html_elements(x = dwight, css = ".diet-title a") %>%
  html_text2()
speaker
```
    
---

# Title

```{r get-title, dependson = "get-statement"}
title <- html_elements(x = dwight, css = "h1") %>%
  html_text2()
title
```

---

# Text

```{r get-text, dependson = "get-statement"}
text <- html_elements(x = dwight, css = "div.field-docs-content") %>%
  html_text2()

# This is a long document, so let's just display the first 1,000 characters
text %>% str_sub(1, 1000) 
```
    
---

# Make a function

Make a function called `scrape_docs` that

- Accepts a URL of an individual document
- Scrapes the page
- Returns a data frame containing the document's
    - Date
    - Speaker
    - Title
    - Full text

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 5)
```

---

# Random observations on scraping

- Make sure you've obtained only what you want
- Store all your `read_html()` calls
- If you are having trouble parsing, try selecting a smaller subset of the thing you are seeking
- Confirm that there is no R package and no API
